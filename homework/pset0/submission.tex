\documentclass[12pt]{exam}

\usepackage[utf8]{inputenc}  % For UTF8 source encoding.
\usepackage{amsmath}  % For displaying math equations.
\usepackage{amsfonts} % For mathematical fonts (like \mathbb{E}!).
\usepackage{upgreek}  % For upright Greek letters, such as \upvarphi.
\usepackage{wasysym}  % For additional glyphs (like \smiley!).
\usepackage{hyperref}
% For document margins.
\usepackage[left=.8in, right=.8in, top=1in, bottom=1in]{geometry}
\usepackage{lastpage} % For a reference to the number of pages.

% TODO: Enter your name here :)
\newcommand*{\authorname}{[Luis A. Perez]}

\newcommand*{\psetnumber}{0}
\newcommand*{\psetdescription}{Concept Refresher}
\newcommand*{\duedate}{Tuesday, April 9th}
\newcommand*{\duetime}{2:30 pm}

% Fancy headers and footers
\headrule
\firstpageheader{CS166\\Spring 2019}{Problem Set \psetnumber\\\psetdescription}{Due: \duedate\\at \duetime}
\runningheader{CS166}{Problem Set \psetnumber}{\authorname}
\footer{}{\footnotesize{Page \thepage\ of \pageref{LastPage}}}{}

% Exam questions.
\newcommand{\Q}[1]{\question{\large{\textbf{#1}}}}
\qformat{}  % Remove formatting from exam questions.

% Useful macro commands.
\newcommand*{\ex}{\mathbb{E}}
\newcommand*{\bigtheta}[1]{\Theta\left( #1 \right)}
\newcommand*{\bigo}[1]{O \left( #1 \right)}
\newcommand*{\bigomega}[1]{\Omega \left( #1 \right)}
\newcommand*{\prob}[1]{\text{Pr} \left[ #1 \right]}
\newcommand*{\var}[1]{\text{Var} \left[ #1 \right]}

% Custom formatting for problem parts.
\renewcommand{\thepartno}{\roman{partno}}
\renewcommand{\partlabel}{\thepartno.}

% Framed answers.
\newcommand{\answerbox}[1]{
\begin{boxed}
\hspace{\fill}
\vspace{#1}
\end{boxed}}

% MZ
\usepackage{amsthm}
\usepackage{amssymb}
\let\oldemptyset\emptyset
\renewcommand{\emptyset}{\text{\O}}
\renewcommand\qedsymbol{$\blacksquare$}
\newenvironment{prf}{{\bfseries Proof.}}{\qedsymbol}
\newcommand{\bi}[1]{\textit{\textbf{#1}}}
\newcommand{\annotate}[1]{\textit{\textcolor{blue}{#1}}}
\usepackage{stmaryrd}
\makeatletter
\@namedef{ver@framed.sty}{9999/12/31}
\@namedef{opt@framed.sty}{}
\makeatother
\usepackage{minted}
\usepackage{mathtools}
\usepackage{alltt}

\printanswers

\setlength\answerlinelength{2in} \setlength\answerskip{0.3in}

\begin{document}
\title{CS166 Problem Set \psetnumber: \psetdescription}
\author{\authorname}
\date{}
\maketitle
\thispagestyle{headandfoot}

\begin{center}
\section*{Section One: Mathematical Prerequisites}    
\end{center}

\begin{questions}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\Q{Problem One: Fibonacci Fun! (3 Points)}

The Fibonacci numbers are a famous sequence defined as
\[
  F_0 = 0 \qquad F_1 = 1 \qquad F_{n+2} = F_n + F_{n+1}
\]

For example, the first few terms of the Fibonacci sequence are
\[
  0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, \ldots
\]

There's a close connection between the Fibonacci numbers and the quantity $\upvarphi=\frac{1+\sqrt{5}}{2}$, the \bi{golden ratio}. In case you're wondering where that number comes from, the golden ratio is the positive root of the quadratic equation $x^2 = 1 + x$.

We'd like you to prove some results about the Fibonacci numbers. In what follows, please do not use any properties of Fibonacci numbers other than what's given in the definition above. The purpose of this problem is to make sure you're comfortable reasoning about terms from first principles.

\begin{parts}

\part Using the formal definition of big-O notation, prove that $F_n = \bigo{\upvarphi^n}$. To do so, find explicit choices of the constants $c$ and $n_0$ for the definition of big-O notation, then use induction to prove that those choices are correct.

Our proofwriting style expectations are along the lines of what you'd see in CS161. Write in complete sentences rather than bullet points, use mathematical notation when appropriate but not as a stand-in for plain English, etc. Remember that an actual person is going to be reading your proof, so be nice to them by writing a lucid, clear argument that respects the intelligence of the reader but doesn't ask them to do the heavy lifting for you. \smiley

\begin{solution}
To formally prove that $F_n = \bigo{\upvarphi^n}$, we must show that $\exists n_0 \in \mathbb{N}, \exists c \in \mathbb{R}$ such that $\forall n \in \mathbb{N}$, if $n \geq n_0$, then $F_n \leq c\upvarphi^n$. We will do this by strong induction on $n$ using the constants $n_0 = 0$ and $c = 1$. We arrived at these constants by attempting the proof and working out the values. We begin with two base cases: $n = 0$ and $n = 1$. We have:
\begin{align*}
  F_0 &= 1  \leq \upvarphi^0 = 1 \\
  F_1 &= 1  \leq \upvarphi^1 = \frac{1 + \sqrt{5}}{2} \approxeq 1.618 
\end{align*}
We now state formally the strong inductive hypothesis. Let us assume that for all $n \leq k$, the following inequality holds (for an example, our base-cases proves this for $k = 1$):
$$
F_n \leq c \upvarphi^n = \upvarphi^{n}
$$
We now seek to demonstrate that given the above, for $n = k + 1$, the following also holds:
$$
F_{k+1} \leq \upvarphi^{k+1}
$$
As we'll see below, this eventually boils down to solving the following inequality for $c$:
\begin{align*}
c\upvarphi^{k+1} &\leq c \upvarphi^k + c \upvarphi^{k-1} \\
\implies c\upvarphi^{2} &\leq c[\upvarphi + 1] 
\end{align*}
which turns out to hold true for all values of $c$, and in particular, for $c = 1$, since in that case we simply have the statement $\upvarphi^2 \leq \upvarphi + 1$.
As we see above, any $c$ satisfying the restriction of big-O as well as large-enough to satisfy our base-cases will work. Specifically, we must have $c \in [1, \infty)$ for $n_0 = 0$. We pick $c = 1$ for simplicity.
\begin{align*}
F_{k+1} &= F_{k} + F_{k-1} \tag{By definition of $F_n$} \\
&\leq \upvarphi^{k} +  \upvarphi^{k-1} \tag{By strong induction} \\
&= \upvarphi^{k-1}[\upvarphi + 1] \tag{Factoring out $\upvarphi^{k-1}$} \\
&= \upvarphi^{k-1}\upvarphi^2 \tag{$\upvarphi + 1 = \upvarphi^2$} \\
&= \upvarphi^{k+1}
\end{align*}
Therefore, given our strong inductive hypothesis, we have show that $F_{k+1} \leq \upvarphi^{k+1}$.

Putting it all together, the above shows that for all $n \geq n_0$, $F_{n} \leq c \varphi^{n}$ where $n_0 = 0$ and $c = 1$. We conclude that $F_n = \bigo{\upvarphi^n}$.
\end{solution}

\pagebreak

\part Along the lines of part (i) of this problem, using the formal definition of big-$\Omega$ notation, prove that $F_n = \bigomega{\upvarphi^n}$.

\begin{solution}
To formally prove that $F_n = \bigomega{\upvarphi^n}$, we must show that $\exists n_0 \in \mathbb{N}, \exists c \in \mathbb{R}$ such that $\forall n \in \mathbb{N}$, if $n \geq n_0$, then $F_n \geq c\upvarphi^n$. We will do this by strong induction on $n$ using the constants $n_0 = 0$ and $c = \frac{1}{\upvarphi}$. We arrived at these constants by attempting the proof and working out the values. We begin with two base cases: $n = 0$ and $n = 1$. We have:
\begin{align*}
  F_0 &= 1 \geq \frac{1}{\upvarphi} \upvarphi^0 = \frac{1}{\upvarphi} \approxeq 0.618 \\
  F_1 &= 1 \geq \frac{1}{\upvarphi} \upvarphi^1 = 1 
\end{align*}
We now state formally the strong inductive hypothesis. Let us assume that for all $n \leq k$, the following inequality holds (for an example, our base-cases proves this for $k = 1$):
$$
F_n \geq c \upvarphi^n = \frac{1}{\upvarphi} \upvarphi^{n}
$$
We now seek to demonstrate that given the above, for $n = k + 1$, the following also holds:
$$
F_{k+1} \geq \frac{1}{\upvarphi} \upvarphi^{k+1}
$$
As we'll see below, this eventually boils down to solving the following inequality for $c$:
\begin{align*}
c\upvarphi^{k+1} &\geq c \upvarphi^k + c \upvarphi^{k-1} \\
\implies c\upvarphi^{2} &\geq c[\upvarphi + 1] 
\end{align*}
which turns out to hold true for all values of $c$, and in particular, for $c = \frac{1}{\upvarphi}$.
As we see above, any $c$ satisfying the restriction of big-$\Omega$ as well as small-enough to satisfy our base-cases will work. Specifically, we must have $c \in (0, \frac{1}{\upvarphi}]$ for $n_0 = 0$. We pick $c = \frac{1}{\upvarphi}$ for simplicity.
\begin{align*}
F_{k+1} &= F_{k} + F_{k-1} \tag{By definition of $F_n$} \\
&\geq \frac{1}{\upvarphi} \upvarphi^{k} +  \frac{1}{\upvarphi} \upvarphi^{k-1} \tag{By strong induction} \\
&= \frac{1}{\upvarphi} \upvarphi^{k-1}[\upvarphi + 1] \tag{Factoring out $\upvarphi^{k-1}$} \\
&= \frac{1}{\upvarphi}\upvarphi^{k-1}\upvarphi^2 \tag{$\upvarphi + 1 = \upvarphi^2$} \\
&= \frac{1}{\upvarphi} \upvarphi^{k+1}
\end{align*}
Therefore, given our strong inductive hypothesis, we have show that $F_{k+1} \geq \frac{1}{\upvarphi} \upvarphi^{k+1}$.

Putting it all together, the above shows that for all $n \geq n_0$, $F_{n} \geq c \varphi^{n}$ where $n_0 = 0$ and $c = \frac{1}{\upvarphi}$. We conclude that $F_n = \bigomega{\upvarphi^n}$.
\end{solution}

\end{parts}

You've just proved that $F_n = \Theta(\upvarphi) = \Theta(\upvarphi^n)$, which is not immediately obvious! Fibonacci numbers show up in lots of algorithms and data structures, and what you've just proved will definitely make an appearance later this quarter.

\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\Q{Problem Two: Probability and Concentration Inequalities (4 Points)}

The analysis of randomized data structures sometimes involves working with sums of random variables. Our goal will often be to get a tight bound on those sums, usually to show that some runtime is likely to be low or that some estimate is likely to be good. If we only know two pieces of information about those random variables (what their expected values are and that they're nonnegative), we can get some information about how their sums behave.

\begin{parts}

\part Let $X_1, X_2, \ldots, X_n$ be a collection of $n$ nonnegative random variables such that $\ex\left[X_i\right] = 1$ for each variable $X_i$. (Note that these random variables might not be independent of one another.) Prove that $\prob{\sum_{i=1}^n X_i \geq 2n} \leq \frac{1}{2}$. You may want to use Markov's inequality.

\begin{solution}
Let us begin by considering the random variable $Y$ defined as $Y = \sum_{i=1}^{n} X_i$ (eg, the sum of our collection of $X_i$). We compute $\ex[Y]$ first.
\begin{align*}
\ex[Y] &= \ex\left[\sum_{i=1}^n X_i\right] \tag{Definition of $Y$} \\
&= \sum_{i=1}^n \ex[X_i] \tag{Linearity of expectation, which holds for any r.v} \\
&= \sum_{i=1}^n 1 \tag{$\ex[X_i] = 1$ as given in the problem} \\
&= n
\end{align*}
We now note that $Y$ is a non-negative random variable (it is the sum of non-negative random variables) and has a finite-expected value. We can apply Markov's inequality, and do so with $c = 2$ to have the following:
\begin{align*}
\prob{Y \geq 2 \ex[Y]} \geq \frac{1}{2} \tag{Markov Inequality applied to $Y$ with $c = 2$} \\
\iff \prob{\sum_{i=1}^n X_i \geq 2n} \leq \frac{1}{2} \tag{Definition of $Y$ and results from above}
\end{align*}
The above proves the statement we wanted to prove.
\end{solution}

\uplevel{Sometimes you'll find that the sort of bound you get from an analysis like part (i) isn't strong enough to prove what you need to prove. In those cases, you might want to start looking more at the spread of each individual random variable. If, for example, you know the variances of those variables are small, you might be able to get a tighter bound.}

\part Let $X_1, X_2, \dots, X_n$ be a collection of $n$ nonnegative random variables. As in part (i), you know that $\ex[X_i] = 1$ for each variable $X_i$. But now suppose you know two other facts. First, you know that each variable has unit variance (the fancy way of saying $\var{X_i} = 1$ for each variable $X_i$). Second, while you don't know for certain whether these variables are independent of one another, you know that they're \bi{pairwise uncorrelated}. That is, you know that $X_i$ and $X_j$ are uncorrelated random variables for any $i \neq j$. Under these assumptions, prove that $\prob{\sum_{i=1}^n X_i \geq 2 n} \leq \frac{1}{n}$. You may want to use Chebyshev's inequality. 

\begin{solution}
We follow a similar approach to the previous part, with the additional assumption that $n > 0$ (this is not given in the problem statement, but without this restriction, the given inequality is undefined). In particular, we again define the random variable $Y = \sum_{i=1}^{n} X_i$ which is a non-negative random variable. From the part above, we already know that $\ex[Y] = n$. Since we know $\var{X_i} = 1$ and that the variables are pairwise uncorrelated, we begin by first using this information to find $\var{Y}$.
\begin{align*}
  \var{Y} &= \var{\sum_{i=1}^n X_i} \\
  &= \sum_{i=1}^n \var{X_i} + \sum_{i \neq j} \text{Cov}[X_i, X_j] \tag{Properties of variance of any set of random variables} \\
  &=  \sum_{i=1}^n \var{X_i} \tag{Variables are pair-wise uncorrelated, so $\text{Cov}[X_i, X_j] = 0$ for $i \neq j$} \\
  &= n \tag{$\var{X_i} = 1$}
\end{align*}
We now have enough information to prove the given statement. We begin by manipulating it a little bit.
$$
 \prob{\sum_{i=1}^n X_i \geq 2n} = \prob{\sum_{i=1}^n X_i - n \geq n}
$$
From here, we note that we can apply an upper bound. Let $A$ be the event where $\sum_{i=1}^n X_i - n \geq n$ and let $B$ be the event where $|\sum_{i=1}^n X_i - n| \geq n$. Then note that $A \implies B$, since we know that $n > 0$. As such, If $A$ occurs, then we know that $\sum_{i=1}^n X_i - n$ is positive, which implies $B$ occurs. This means that $\prob{B} \geq \prob{A}$. Fully written out, we know:
$$
\prob{\sum_{i=1}^n X_i - n \geq n} \leq \prob{|\sum_{i=1}^n X_i - n| \geq n}
$$
We can now make some subtitutions to make this resemble Chebyshev's inequality and thereby make use of that known result. In particular, recall that $Y = \sum_{i=1}^n X_i$, $\ex[Y] = n$, and $\var{Y} = n$. We therefore have:
\begin{align*}
\prob{|\sum_{i=1}^n X_i - n| \geq n} = \prob{|Y - \ex[Y]| \geq \sqrt{n}\sqrt{\var{Y}}} \\
\leq \frac{1}{n} \tag{By Chebyshev's inequality with $c = \sqrt{n} > 0$}
\end{align*}
Putting it all together, for non-negative, pair-wise uncorrelated $X_i$ with unit variance we have:
$$
\prob{\sum_{i=1}^n X_i \geq 2n} \leq \frac{1}{n}
$$
\end{solution}

\uplevel{The analysis in part (ii) only works if the variables are pairwise uncorrelated.}

\part Pick a natural number $n > 0$ and define a collection of random variables $X_1, X_2, \ldots, X_n$ such that
\begin{itemize}
\item each $X_i$ is nonnegative,
\item $\ex[X_i] = 1$ for each variable $X_i$,
\item $\var{X_i} = 1$ for each variable $X_i$, but
\item $\prob{\sum_{i=1}^n X_i \geq 2n} > \frac{1}{n}$.
\end{itemize}

Once you've done this, go back to your proof from part (ii) and make sure you can point out the specific spot where the math breaks down once you remove the requirement that the Xi's be pair- wise uncorrelated.

\begin{solution}
We take $n = 3$. Let $X_1, X_2, X_3$ be the random variables taking on two values, $0$ and $2$ with equal probability. It is straight-forward to confirm that each $X_i$ is non-negative, $\ex[X_i] = 1$ (the average of the two values), and that $\var{X_i} = \ex[(X - \ex[X_i])^2] = 1$ (distance from the expected value is always $1$). However, we correlate $X_i$ such that $X_i = X_j$ for all $i,j$. We therefore have:
$$
\prob{\sum_{i=1}^3 X_i \geq (2)(3)} = \frac{1}{2} \geq \frac{1}{3}
$$
This is because the sum of the variables can be either $0 + 0 + 0$ or $2 + 2 + 2 = 6$, each event with equal probability. Note that this easily extends for arbitrary values of $n$. This, however, does not contradict our proof from above since this breaks the assumption we made where $\text{Cov}[X_i, X_j] = 0$ for $i \neq j$. In fact, with the random variables as defined, we have that $\text{Cov}[X_i, X_j] = 1$ for all $i,j$.
\end{solution}

\uplevel{As you saw in this problem, learning more about the distribution of random variables makes it easier to provide tighter bounds on their sums, and correlations across those variables makes it harder. This is a good intuition to have for later in the quarter, where we'll be discussing how different assumptions on hash functions lead to different analyses of data structures.}

\end{parts}

\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{center}
\section*{Section Two: Algorithmic Prerequisites}    
\end{center}

\Q{Problem Three: Binary Search Trees (4 Points)}

This one is all coding, so you don't need to write anything here. Make sure to submit your final implementation on myth.
\vspace{.2 in}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\Q{Problem Four: Event Planning (4 Points)}

You're trying to figure out what Fun and Exciting Things you'd like to do over the weekend. You download a list of all the local events going on in your area. Each event is tagged with its location, which you can imagine is a point in the 2D plane. (We'll pretend that the world is flat, at least in a small neighborhood around your location. Thanks, multivariable calculus.) You also have your own $(x, y)$ location. 

Design a algorithm that, given some number $k$, returns a list of the $k$ events that are closest to you, sorted by increasing order of distance. Your algorithm should run in time $\bigo{n + k \log k}$, where $n$ is the number of nearby events. Then prove your algorithm is correct and meets the required time bounds.

Some specific details and edge cases to watch for:
\begin{itemize}
\item You can assume, for simplicity, that no two events are at the same distance from you.
\item By ``distance'', we mean Euclidean distance. We're already assuming the world is flat, so while we're at it seems pretty reasonable to also ignore things like roads and speed limits. \smiley
\end{itemize}

As a hint, think about the algorithms you studied in CS161 and see if any of them would make for good subroutines.

To make things easier for the grader, we recommend doing the following when writing up your solution:
\begin{enumerate}
    \item Start off by giving a quick, two-sentence, high-level description of your approach. This makes it easier for the grader to contextualize what it is that you're trying to do. 
    \item Next, go into more detail. Describe how your algorithm works, one step at a time. Please don't write actual code unless it's exceptionally well-commented and serves a purpose that plain English couldn't. (Trust us -- from experience, reading code is often much harder than reading prose!)
    \item Write a quick correctness proof. Tell us what, specifically, you're going to prove, then go prove it. Our proof expectations are similar to those for CS161 -- write in complete sentences, use mathematical notation when appropriate but not as a stand-in for plain English, etc. 
    \item Write a runtime analysis. Go at whatever level of detail seems most appropriate.
\end{enumerate}

A note on this problem, and other problems going forward: when measuring runtime in the context of algorithms and data structures, it's important to distinguish between \bi{deterministic} and \bi{randomized} algorithms. There's a lot of research into how to take \emph{randomized} algorithms with a nice \emph{expected} runtime and convert them into \emph{deterministic} algorithms with a nice \emph{worst-case} runtime. Since this problem set is designed as a warm-up, we'll accept either a deterministic algorithm with a worst-case runtime of $O(n + k \log k)$ or a randomized algorithm with an expected runtime of $\bigo{n + k \log k}$, though in the future we'll tend to be a bit stricter about avoiding randomness.

\begin{solution}

\textbf{Overview}
The main idea of the algorithm is to first construct a min-heap of the events (ordered by distance) and then use a secondary min-heap to efficiently compute the minimum $k$ distance events. We then return these $k$ events, which will be sorted and will be the closest. We assume that computing the Eucledian distance is $\bigo{1}$. With the use of these two min-heaps, the algorithm will run in $\bigo{n + k \log k}$ time. 

\textbf{Representation}
We will keep a few data-structures in-memory. A hash-map \textit{events}, two max-heaps, \textit{primary} and \textit{secondary}, and a final array \textit{closestEvents}.

\textbf{Invariants}
The main invarient of interest occurs when adding elements to \textit{closestEvents}. The invariant we hold is that at all times, if \textit{closestEvents}$[i]$ has been added, then it is the $(i+1)$-th closests event. 

\textbf{Algorithm Details}
We now describe the algorithm in more detail.

The first-step is to construct the hash-map \textit{events}. This is done by simply iterating over the input array, and for each element, computing the distance to the event (from our current position), and using this distance as the key in the hashmap. The value is the event itself. This can be done in $\bigo{n}$ time, since for each event we do $\bigo{1}$ work. Keys are unique since, as per the problem statement, we can assume each event is a distinct distance from us.

The next-step is to construct \textit{primary}, a min-heap out of the keys in \textit{events} (eg, the distances of events). We use the min-heapify algorithm, which has run-time $\bigo{n}$ (See \href{https://web.stanford.edu/class/archive/cs/cs161/cs161.1168/lecture4.pdf}{notes} from CS161).

Once \textit{primary} is constructed, we proceed with the interesting parts of the algorithm, which consists of filling up \textit{closestEvents}. We begin by peeking at the minimum element in our \textit{primary} min-heap and adding this element to our \textit{secondary} min-heap (including keeping track of the left and right children, but not adding these). We now enter our for-loop.

Starting with $i = 0 \to k - 1$, we first extract the minimum node from \textit{secondary}. Use the stored-information within the node about the left and right children in the \textit{primary} min-heap, and insert thse nodes (if they exist, along with information about their left and right children) into our \textit{secondary} min-heap. Use \textit{events} map to find the event corresponding to the extracted node from our \textit{secondary} min-heap, and place this event in \textit{closetsEvents}$[i]$.

At the end of the loop above, return \textit{closestEvents}.

\textbf{Proof Of Correctness}
We now prove that at the end of the algorithm, \textit{closestEvents} consists of the $k$ closests evensts in ascending order of distance. 

The simplest way to proof this is by establishing a few properties of our min-heaps, and then proving our extraction loop using induction.
 
We recall that a min-heap has the property that for any node $n$ (other than the root), the $parent(n) \leq n$. In our case, this inequality is strict (since all distances are distinct). From this property, it follows that the root is the minimum element. We can use this property directly to establish our base case.

We wish to establish the following loop-invarients. At the beginning of the loop execution ($i = 1 \cdots k$), we have:
\begin{enumerate}
  \item \textit{secondary} contains the $i$-th smallest distance, and this is its root element.
  \item \textit{closestsEvents}$[0 \cdots i-2]$ contains the top-$(i-1)$ elements, in ascending order. For $i = 1$, the array is empty.
  \item The top $i-1$ elements and all of their immediate children have been inserted into \textit{secondary} at some point. If $i = 1$, this consists of just the smallest element.
\end{enumerate}

The base case begins with $i = 1$. Before our first-loop execution, we note that \textit{secondary} contains one-element, the root of the \textit{primary} min-heap. As such, invariant (1) holds, since by the properties of min-heaps, this is the closests distance. Property (2) holds vacuously, since there are no elements in the described set. Similarly, property (3) holds since the smallest element has been inserted (before loop execution).

We now assume that the above invariants hold at the beginning of execution $i$. We wish to show that they still-hold at the beginning of the next execution $i + 1$ (alternatively, that they hold at the end of the $i$-th execution of the loop).

We begin by showing that invariant (2) holds at the beginning of the $(i + 1)$-th execution if all-invariants above held at the beginning of the $i$-th execution.

Consider the $i$-th execution of the for-loop. First, the algorithm extracts the root from \textit{secondary} and adds the corresponding event to \textit{closestEvents}$[i-1]$. By invariant (1), this newly added element is the $i$-th smallest, and it is added at position $i - 1$. Combined with invariant (2), this means that at the beginning of the $(i+1)$-th execution, \textit{closetsEvents}$[0 \cdots i-1]$ contains the top-$i$ elements, in sorted order, thereby satisfying invariant (2) at the beginning of the $(i+1)$-th execution.

Next, we show that invariant (3) holds at the beginning of the $(i + 1)$-th execution if all invariants above held at the beginning of the $i$-th execution.

We note that during the execution of the loop, we insert the children of the $i$-th smallest value into \textit{secondary} (this follows from (1)). Combined with (3), this means that at the end of the $i$-th execution, the top $i$ elements and all of their immediate children have been inserted into \textit{secondary}.

Finally, we show that invariant (1) holds, which is the critical invarient. 

Again, consider the $i$-th execution of the for-loop. We want to show that after this execution is complete, \textit{secondary} contains the $(i+1)$-th smallest element (if one exists) and that this element is the root. By (3), we know that we've inserted the top $i$ elements and all of their immediate children to \textit{secondary} at the end of the $i$-th exectuon. The first claim is that the $(i+1)$-th element (if it exists) must be a child of the top $i$ elements (and as such, has been inserted into \textit{secondary}). Suppose this were not the case. Then that means that the $(i+1)$-th smallest element, $C$, is the child of a node whose value is not in the top $i$ elements, $P$. However, by the min-heap property, it must be the case that the value of $P$ is smaller than that of $C$ (strictly smaller in our-case), which contradicts the fact that $P$ is the $(i+1)$-th smallest node. Therefore, by contradiction, it must be the case that the $(i+1)$-th smallest node is a child of the top $i$ elements.

Combined with (3), this means this node has been inserted into \textit{secondary}. By the min-heap property, now that we have removed the root node (which was the $i$-th smallest element), it must be the case that the node containing the $(i+1)$-th element is now the root node by (3). This shows that after the loop execution, invariant (1) still holds.


Combining all of the above, we have that after $k$ execution of the for loop (beginning of the $k+1$), the following invariants hold:
\begin{enumerate}
  \item \textit{secondary} contains the $k+1$-th smallest distance, and this is its root element.
  \item \textit{closestsEvents}$[0 \cdots k-1]$ contains the top-$k$ elements, in ascending order.
  \item The top $k$ elements and all of their immediate children have been inserted into \textit{secondary} at some point.
\end{enumerate}
In particular, since (2) holds, this shows that our algorithm is correct.

\textbf{Runtime Analysis}

We now analyze the run-time of the proposed algorithm. Constructing the hash-map \textit{events} takes $\bigo{n}$ time. Constructing the min-heap \textit{primary}, by the results from CS161, takes $\bigo{n}$ time. We then iterate $k$ times, each time extracting one element and adding two elements to \textit{secondary} (plus some constant amount of work to lookup the event and move it to the output array). We first note that since at each iteration we extract an element and add two, the size of \textit{secondary} is $\bigo{k}$. We then recall that extraction from a min-heap of size $k$ is $\bigo{\log k}$ time, and insertion is $\bigo{\log k}$ time. As such, this part of the algorithm takes $\bigo{k \log k}$ time. 

Combining the two results above, our algorithm runs in $\bigo{n + k \log n}$.

\end{solution}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{questions}
\end{document}
